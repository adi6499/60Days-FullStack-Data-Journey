# Day 2 – Gradient Boosting (XGBoost)

### Concept Summary
- Boosting = sequential ensemble method.
- Each new model learns from previous errors.
- XGBoost = eXtreme Gradient Boosting → faster & regularized.

### Results
- Accuracy: ~96-100%
- Strong model with feature importance visualization.

### Key Difference
| Bagging (Random Forest) | Boosting (XGBoost) |
|--------------------------|--------------------|
| Models built in parallel | Models built sequentially |
| Reduces variance | Reduces bias |
| Independent trees | Dependent trees (each fixes previous errors) |

### Reference
- [Krish Naik - XGBoost](https://www.youtube.com/watch?v=OtD8wVaFm6E)
- [GeeksforGeeks - XGBoost](https://www.geeksforgeeks.org/xgboost-algorithm-in-machine-learning/)
